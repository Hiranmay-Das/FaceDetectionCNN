{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Module Imports\n",
    "import os\n",
    "import numpy as np\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Imports\n",
    "import cv2\n",
    "from keras import Input\n",
    "from keras.backend import abs as kerasAbs\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout, Lambda, BatchNormalization, Activation\n",
    "from matplotlib import pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI Imports\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from tkinter.messagebox import showerror\n",
    "\n",
    "root = Tk()\n",
    "root.withdraw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MT_CNN(img, min_face_size=20, scale_factor=0.709):\n",
    "    \"\"\"\n",
    "    Reads an Image and detects the faces in this image using\n",
    "    a Pre-Trained Multi-task Cascaded Convolutional Neural Networks for Face Detection,\n",
    "    based on TensorFlow.\n",
    "\n",
    "    --Parameters--\n",
    "    img - Matrix of the type CV_8U containing an image where objects are detected.\n",
    "    min_face_size: Minimum possible object size. Objects smaller than that are ignored.\n",
    "    scale_factor: Parameter specifying how much the image size is reduced at each image scale.\n",
    "\n",
    "    \"\"\"\n",
    "    model = MTCNN(min_face_size=min_face_size, scale_factor=scale_factor)\n",
    "    faces = model.detect_faces(img)\n",
    "    faces = filter(lambda x: x['confidence'] >= 0.90, faces)\n",
    "    return list(map(lambda x: x['box'], faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, required_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Resize the image to the required dimensions.\n",
    "    \"\"\"\n",
    "    return cv2.resize(img, required_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo():\n",
    "    \"\"\"\n",
    "    Accepts a photo from the user and returns it as a numpy array\n",
    "    \"\"\"\n",
    "    filename = askopenfilename(\n",
    "        title=\"Select Test Image\",\n",
    "        filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png;\")],\n",
    "        initialdir=os.getcwd(),\n",
    "        parent=root\n",
    "    )\n",
    "    if filename:\n",
    "        image = cv2.imread(filename)\n",
    "    else:\n",
    "        showerror(\"Error\", \"No File Selected.\")\n",
    "        raise SystemExit\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_face(image, face, predicted, rect_color=(0, 255, 0), text_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Labels the face in the image with the predicted label\n",
    "    image - the image to be labeled\n",
    "    face - the face to be labeled\n",
    "    predicted - the label to be applied\n",
    "    rect_color - the color of the rectangle\n",
    "    text_color - the color of the text\n",
    "    \"\"\"\n",
    "    x, y, w, h = face\n",
    "    font_size, font_thiccness = 0.7, 1\n",
    "    l, b = cv2.getTextSize(\n",
    "        predicted, cv2.FONT_HERSHEY_SIMPLEX, font_size, font_thiccness)[0]\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), rect_color, 2)\n",
    "    cv2.rectangle(image, (x-1, y), (x+max(l, w)+1, y-b-10), rect_color, -1)\n",
    "    cv2.putText(image, predicted, (x, y-6),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, text_color, font_thiccness)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_faces(image):\n",
    "    \"\"\"\n",
    "    Detects faces in the image and returns their locations\n",
    "    image - the image to be analyzed\n",
    "    \"\"\"\n",
    "    faces_loc = MT_CNN(image, min_face_size=60, scale_factor=0.747)\n",
    "    return faces_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_faces(image, faces_loc):\n",
    "    \"\"\"\n",
    "    Crops the faces from the image and returns their data\n",
    "    image - the image to be analyzed\n",
    "    faces_loc - the location of the faces\n",
    "    \"\"\"\n",
    "    faces_data = []\n",
    "    for (x, y, w, h) in faces_loc:\n",
    "        face = image[y:y+h, x:x+w]\n",
    "        face = resize_img(face)\n",
    "        faces_data.append(face)\n",
    "\n",
    "    return faces_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEW SHOT LEARNING TRAINING AND TESTING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# N-way K-shot Learning\n",
    "N = 6 # number of classes\n",
    "K = 5 # number of images per class\n",
    "LIMIT = 8 # number of images to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path=None, label=0, mode=None):\n",
    "    \"\"\"\n",
    "    Loads images from the given path and returns a list of images.\n",
    "    path - the path from which to load the images\n",
    "    label - the label of the images\n",
    "    mode - the mode of the training.\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is not defined\")\n",
    "\n",
    "    if mode is None:\n",
    "        mode = 0\n",
    "\n",
    "    elif isinstance(mode, int):\n",
    "        if mode not in [0, 1]:\n",
    "            raise ValueError(\"Mode should be either 0 or 1\")\n",
    "\n",
    "    elif isinstance(mode, str):\n",
    "        if mode not in [\"train\", \"val\"]:\n",
    "            raise ValueError(\"Mode should be either 'train' or 'val'\")\n",
    "        mode = {\"train\":0, \"val\":1}[mode]\n",
    "\n",
    "    classes = []\n",
    "    labels = []\n",
    "    current_label = label\n",
    "    classes_dict = dict()\n",
    "    target = 0\n",
    "    valid_set = [{\"Hiranmay\", \"Neel\", \"Shilpi\"}, {\"Shreya\", \"Srijani\", \"Richa\"}][mode]\n",
    "    \n",
    "    for person in os.scandir(path):\n",
    "        if person.is_dir():\n",
    "            if person.name not in valid_set:\n",
    "                continue\n",
    "            print(f\"Loading images from {person.name}\")\n",
    "            classes_dict[person.name] = [current_label, current_label]\n",
    "            person_images = []\n",
    "            for img_no, image in enumerate(os.scandir(person.path)):\n",
    "                if image.is_file():\n",
    "                    if img_no == LIMIT:\n",
    "                        break\n",
    "                    img = cv2.imread(image.path)\n",
    "                    person_images.append(img)\n",
    "                    labels.append(target)\n",
    "                    classes_dict[person.name][1] = current_label\n",
    "                    current_label += 1\n",
    "            target += 1\n",
    "            classes.append(np.stack(person_images))\n",
    "    labels = np.vstack(labels)\n",
    "    classes = np.stack(classes)\n",
    "\n",
    "    return classes, labels, classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, c_train = load_images(\"dataset/train/labelled data\", mode = 0)\n",
    "X_val, y_val, c_val = load_images(\"dataset/train/labelled data\", mode = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_images():\n",
    "    \"\"\"\n",
    "    Tests the load_images function\n",
    "    \"\"\"\n",
    "    mode = 0\n",
    "    X = [X_train, X_val][mode]\n",
    "    y = [y_train, y_val][mode]\n",
    "    targets = ['Hiranmay', 'Neel', 'Richa', 'Shilpi', 'Shreya', 'Srijani']\n",
    "    fig, axes = plt.subplots(nrows=N//2, ncols=LIMIT, figsize=(28, 28))\n",
    "    for id, person, label in zip(range(N//2), X, y):\n",
    "        for idx, img in enumerate(person, start = 0):\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            axes[id, idx].imshow(img)\n",
    "            axes[id, idx].set_title(f\"{targets[id]} {idx+1}\")\n",
    "            axes[id, idx].axis('off')\n",
    "    plt.show()\n",
    "test_load_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(batch_size, mode = None):\n",
    "    \"\"\"\n",
    "    Loads a batch of images from the given mode.\n",
    "    batch_size - the size of the batch\n",
    "    mode - the mode of the training.\n",
    "    \"\"\"\n",
    "    if mode is None:\n",
    "        mode = 0\n",
    "\n",
    "    elif isinstance(mode, int):\n",
    "        if mode not in [0, 1]:\n",
    "            raise ValueError(\"Mode should be either 0 or 1\")\n",
    "\n",
    "    elif isinstance(mode, str):\n",
    "        if mode not in [\"train\", \"val\"]:\n",
    "            raise ValueError(\"Mode should be either 'train' or 'val'\")\n",
    "        mode = {\"train\":0, \"val\":1}[mode]\n",
    "        \n",
    "    classes = [X_train, X_val][mode]\n",
    "    n_classes, n_samples, width, height, channels = classes.shape\n",
    "    \n",
    "    random_classes = np.random.choice(n_classes, batch_size)\n",
    "    \n",
    "    training_pairs = [np.zeros((batch_size, width, height, channels)) for _ in range(2)]\n",
    "    training_labels = np.zeros((batch_size,))\n",
    "\n",
    "    training_labels[batch_size//2:] = 1\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        class_id = random_classes[i]\n",
    "        idx_1 = np.random.randint(0, n_samples)\n",
    "        training_pairs[0][i,:,:,:] = classes[class_id, idx_1].reshape(width, height, 3)\n",
    "        \n",
    "        while True:\n",
    "            idx_2 = np.random.randint(0, n_samples)\n",
    "            if idx_2 != idx_1:\n",
    "                break\n",
    "\n",
    "        if i >= batch_size // 2:\n",
    "            class_id2 = class_id \n",
    "        else: \n",
    "            class_id2 = (class_id + np.random.randint(1, n_classes)) % n_classes\n",
    "        training_pairs[1][i] = classes[class_id2, idx_2].reshape(width, height, 3)\n",
    "    \n",
    "    return training_pairs, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_batch():\n",
    "    \"\"\"\n",
    "    Tests the load_batch function\n",
    "    \"\"\"\n",
    "    batch_size = 6\n",
    "    pairs, labels = load_batch(batch_size, mode = 0)\n",
    "    fig, axes = plt.subplots(nrows=batch_size, ncols=2, figsize=(28, 28))\n",
    "    for id, img1, img2 in zip(range(batch_size), pairs[0], pairs[1]):\n",
    "        axes[id, 0].imshow(cv2.cvtColor(img1.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "        axes[id, 1].imshow(cv2.cvtColor(img2.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "        axes[id, 1].set_title(f\"{labels[id]}\")\n",
    "        axes[id, 0].axis('off')\n",
    "        axes[id, 1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "test_load_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape = (160, 160, 3)):\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(64,(5,5),input_shape=input_shape,activation='relu',kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Convolution2D(128,(5,5),kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Convolution2D(128,(5,5),kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(512,activation='sigmoid',kernel_regularizer='l2'))\n",
    "    \n",
    "    left_emb = model(left_input)\n",
    "    right_emb = model(right_input)\n",
    "    \n",
    "    L1_Layer = Lambda(lambda tensors: kerasAbs(tensors[0] - tensors[1]))\n",
    "    L1_Dist = L1_Layer([left_emb,right_emb])\n",
    "    OP = Dense(1,activation='sigmoid',kernel_regularizer='l2')(L1_Dist)\n",
    "    \n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=OP)\n",
    "    \n",
    "    siamese_net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, mode = \"train\"):\n",
    "    \"\"\"\n",
    "    A generator for batches of images.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = load_batch(batch_size, mode)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    Trains the siamese network.\n",
    "    \"\"\"\n",
    "    model = get_siamese_model()\n",
    "    history = model.fit(generate(32, mode = \"train\"),\n",
    "                        steps_per_epoch=20, \n",
    "                        validation_data=generate(32, mode = \"val\"),\n",
    "                        validation_steps=10, \n",
    "                        epochs=50)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name = \"model\"):\n",
    "    \"\"\"\n",
    "    Saves the model to a file\n",
    "    \"\"\"\n",
    "    model.save(f\"{name}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy and loss.\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"Model_Accuracy.png\")\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"Model_Loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train = False):\n",
    "    \"\"\"\n",
    "    Returns a newly trained model if train is set to True, otherwise returns the model from the saved file.\n",
    "    train - Boolean, whether to train the model or to load it from a file.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        model, history = train_model()\n",
    "        evaluate_model(history)\n",
    "    else:\n",
    "        model = load_model(\"OneShotModel_test.h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_support_set(input_shape=(160,160,3)):\n",
    "    \"\"\"\n",
    "    Loads the support set of images\n",
    "    \"\"\"\n",
    "    support_set = [[None, None] for _ in range(N)]\n",
    "    path = os.path.join(os.getcwd(), \"dataset\", \"support_set\")\n",
    "\n",
    "    for idx, file in enumerate(os.scandir(path)):\n",
    "        if file.is_dir():\n",
    "            continue\n",
    "        label = file.name.split('.')[0]\n",
    "        img = cv2.imread(file.path)\n",
    "        img = resize_img(img, input_shape[:2])\n",
    "\n",
    "        support_set[idx] = [img, label]\n",
    "    \n",
    "    return support_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_faces(image, faces_loc, faces_data):\n",
    "    \"\"\"\n",
    "    Identifies the faces in the image and labels them\n",
    "    image - the image to be labeled\n",
    "    faces_data - the data of the faces\n",
    "    faces_loc - the location of the faces\n",
    "    \"\"\"\n",
    "    model = get_model()\n",
    "    support_set = load_support_set()\n",
    "    for face_loc, face_data in zip(faces_loc, faces_data):\n",
    "        predictions = [model.predict([face_data.reshape(1, 160, 160, 3), support_set[i][0].reshape(1, 160, 160, 3)]) for i in range(N)]\n",
    "        predicted = support_set[np.argmax(predictions)][1]\n",
    "        # cv2.imshow(\"Image\", face_data)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "        # predicted = \"Unknown\"\n",
    "        image = label_face(\n",
    "            image,\n",
    "            face_loc,\n",
    "            predicted,\n",
    "            rect_color=(46, 0, 230),\n",
    "            text_color=(255, 255, 255)\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP1 = perf_counter()\n",
    "image = get_photo()\n",
    "CP2 = perf_counter()\n",
    "faces_loc = locate_faces(image)\n",
    "CP3 = perf_counter()\n",
    "faces_data = encode_faces(image, faces_loc)\n",
    "CP4 = perf_counter()\n",
    "image = identify_faces(image, faces_loc, faces_data)\n",
    "CP5 = perf_counter()\n",
    "print(f'''Time Taken:\n",
    "Load Image = {CP2-CP1} s\n",
    "Locate Faces = {CP3-CP2} s\n",
    "Encode Faces = {(CP4-CP3)*1000} ms\n",
    "Identify Faces = {(CP5-CP4)*1000} ms''')\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite(\"output.jpg\", image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5ea48985cb2ee33f381c055b44e3f84081297de12827903a7542a72f1773939"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
